{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE Sign Language Detector\n",
    "# (Detector de Lengua de Signos Española)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps (Pasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Picture colection\n",
    "- Feature selection: what is important when detecting sign language? \n",
    "    - Hands\n",
    "- Mediapipe test on pictures\n",
    "- Video colection\n",
    "- Video processing\n",
    "    - mediapipe solutions applied to videos\n",
    "    - extraction of keypoints (x, y, z coordinates for positioning and depth)\n",
    "- Data transformation (making sure all data is same shape): \n",
    "    - Adding array of zeros for second hand when only one hand present \n",
    "- Model training (neural net)\n",
    "_________________________________________________________________________________________________________\n",
    "\n",
    "- Recoleccion de imagenes \n",
    "- Selección de variables ¿Que es importante al intentar detectar LSE?\n",
    "    - manos\n",
    "- Prueba de mediapipe solutions en imagenes de LSE\n",
    "- Recoleccion de videos\n",
    "- Procesamiento de video \n",
    "    - aplicación de mediapipe solutions\n",
    "    - extracción de coordenadas de puntos clave de las manos (datos procesados) (coordenadas de posicio, x, y, z)\n",
    "- Transformacion de los datos:\n",
    "    - añadiendo zeros en el array para asegurar que todos tengan la misma forma \n",
    "    - (para compensar por señas que se han con una o dos manos)\n",
    "- Entrenamiento de modelo (red neuronal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description: Three Main Scripts (Descripción: 3 Scripts Principales)\n",
    "\n",
    "The final result of this project will comprise of three main scripts: \n",
    "- <strong> collect_data.py </strong>\n",
    "    - can be run as and when in order to increase amount of actions that the model has learnt to recognise\n",
    "    - this script runs through videos and allows the user to collect a number of frames in the video (corresponding to desired action)\n",
    "- <strong> train.py </strong>\n",
    "    - this can be run to train a new neural network with different parameters or new training data\n",
    "    - it will save the model weights into the model folder \n",
    "- <strong> run_programme.py </strong>\n",
    "    - to use in live video capture or to translate videos of people signing into text \n",
    "    - uses the weights of the desired neural network model \n",
    "\n",
    "______________________________________________________________________________________________________________________________\n",
    "\n",
    "El resultado final del proyecto estara compuesto por tres scripts principales. \n",
    "- <strong> collect_data.py </strong>\n",
    "    - para obtener nuevos datos para posterior entrenamiento del modelo \n",
    "    - aqui se puede incrementar el repertorio de palabras y letras que reconoce el modelo\n",
    "    - el codigo permite visualizar un video y elegir en que momento extraer los puntos claves (en un array) \n",
    "- <strong> train.py </strong>\n",
    "    - para entrenar el modelo, una red neuronal\n",
    "    - los pesos del modelo se guardaran en una carpeta para poder utilizar cuando uno quiera\n",
    "- <strong> run_programme.py </strong>\n",
    "    - en directo o con un video, permite visualizar y aplicar el modelo de deteccion de LSE \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools (Herramientas Utilizadas)\n",
    "### Mediapipe solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MediaPipe Hands = hand and finger tracking solution\n",
    "    hand detection model - drawing the box\n",
    "    hand landmark model -returns keypoints\n",
    "    (solutions provided by google's - open source code)\n",
    "    - STATIC_IMAGE_MODE - False: for video screen, detects and then just tracks \n",
    "    - MAX_NUM_HANDS - defautl: 2\n",
    "    - MAX_NUM_HANDS & MIN_TRACKING_CONFIDENCE - default: 0.5\n",
    "\n",
    "______________________________________________________________________________________________________________________________\n",
    "\n",
    "1. MediaPipe Hands = solución de seguimiento de manos y dedos\n",
    "    modelo de detección manual \n",
    "    modelo de punto de referencia de mano: devuelve puntos clave\n",
    "    (soluciones proporcionadas por Google - código fuente abierto)\n",
    "    - STATIC_IMAGE_MODE - Falso: para la pantalla de video, detecta y luego solo rastrea\n",
    "    - MAX_NUM_MANOS - predeterminado: 2\n",
    "    - MAX_NUM_HANDS & MIN_TRACKING_CONFIDENCE - predeterminado: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29fc651a7b730cd70b5cc0d1ba484563eb58f5d47650e0848ded60a3fec35677"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
